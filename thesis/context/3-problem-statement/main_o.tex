% ------------------------------------------------
\StartChapter{Problem Statement}{chapter:problem-statement}
% ------------------------------------------------

\section{Problem Background}

Graph Neural Networks (GNNs) have achieved remarkable success in a wide range of real-world applications, such as social network analysis, recommender systems, and bioinformatics~\cite{gao2023survey, sharma2024survey, zhang2021graph}. However, their practical performance is highly contingent on the completeness of the input graph data~\cite{Zhang2024, Wu2021}.
In realistic scenarios, graphs are frequently afflicted by missing node features or absent edges, stemming from factors like privacy constraints, sensor failures, or data collection limitations~\cite{jiang2021graph, adhikari2022comprehensive}.
Such missing information fundamentally disrupts the message-passing mechanisms central to GNN architectures, often resulting in suboptimal node representations and diminished performance on downstream tasks.

Missingness in graph data typically manifests in two primary forms:


\begin{itemize}
    \item Uniform missingness refers to randomly distributed missing features throughout the graph, often caused by unpredictable device malfunctions or sporadic data loss.
\end{itemize}

\begin{itemize}
    \item Structural missingness arises when missing data is concentrated within specific subgraphs or clusters of nodes, commonly due to systematic issues such as privacy restrictions or selective sampling policies.
\end{itemize}

In addition to data incompleteness, the scaling of GNNs to large and complex graphs introduces substantial challenges in terms of training cost (computational resources and time) and inference cost (the need for extensive neighborhood aggregation at test time)~\cite{lin2020pagraph, zheng2020distdgl}.
Existing approaches tend to focus on addressing a single aspect—such as feature imputation, model pruning, or architectural tuning—without providing a unified and modular solution that jointly addresses the intertwined challenges of robustness, efficiency, and scalability under incomplete data.

To overcome these practical challenges, a comprehensive framework capable of robust feature imputation, resource-efficient training, and cost-effective inference is essential for deploying GNNs in real-world settings characterized by high levels of missingness and limited computational resources~\cite{xue2023sugar}.


\section{Notations}


\input{./context/3-problem-statement/notations_table}


\section{Formal Statement}

Given a graph $\mathcal{G} = (\mathcal{V}, \mathcal{E}, \mathbf{A}, \mathbf{X})$ with node feature matrix $\mathbf{X} \in \mathbb{R}^{N \times F}$ and a missingness mask $\mathbf{M} \in \{0,1\}^{N \times F}$, the objective of this work is to develop a unified framework that addresses the following intertwined challenges in graph representation learning under incomplete data:


\begin{enumerate}
    \item Robust Feature Imputation:

    Given that a subset of features $x_{i,d}$ is missing (i.e., $\mathbf{M}_{i,d} = 0$), the goal is to recover the missing values $\hat{x}_{i,d}$ such that the imputed feature matrix $\hat{\mathbf{X}}$ maximizes the downstream task performance (e.g., node classification accuracy) under varying missingness patterns (uniform/structural).

    \newpage
    \item Resource-Efficient Training:

    Let $C_{\text{train}}(f)$ denote the computational cost (e.g. Time or FLOPs) of training a GNN-based model $f$ on $\mathcal{G}$. The challenge is to minimize $C_{\text{train}}(f)$ without sacrificing the performance on incomplete graphs, ideally through effective pruning or architectural optimization.

    \item Cost-Effective Inference:

    Define $C_{\text{infer}}(f)$ as the inference cost for deploying $f$ on new, potentially incomplete graphs. The target is to minimize $C_{\text{infer}}(f)$ (e.g., reducing message passing or memory overhead) while ensuring high accuracy and reliability, possibly by leveraging knowledge distillation or graph-to-MLP transformation~\cite{GLNN, KRD, lu2024adagmlp}.

\end{enumerate}

The joint optimization problem underlying our framework can be formalized as follows:

\EquationBegin
\max_{f,\, \hat{\mathbf{X}}} \quad \text{TaskAcc}\big(f(\hat{\mathbf{X}},\, \mathcal{A})\big) \\
\min_{f} \quad C_{\text{train}}(f) \\
\min_{f} \quad C_{\text{infer}}(f) \\
\text{s.t.} \quad \mathbf{M}_{i,d} = 0 \implies \hat{x}_{i,d} \text{ is imputed} \\
\qquad\ \ \text{MissingRate}(\mathbf{M}) \leq \rho
\EquationEnd

Here, the objective is to jointly optimize the model $f$ and the imputed feature matrix $\hat{\mathbf{X}}$ to maximize the downstream task accuracy $\text{TaskAcc}(\cdot)$, subject to constraints on computational costs for both training ($C_{\text{train}}(f)$) and inference ($C_{\text{infer}}(f)$). The model must ensure that every missing entry $(i, d)$ in the original feature mask $\mathbf{M}$ is properly imputed in $\hat{\mathbf{X}}$. Additionally, the overall missing rate of the features, $\text{MissingRate}(\mathbf{M})$, must not exceed a predefined threshold $\rho$.

$\mathcal{A}$ denotes the graph structure (e.g., adjacency matrix), while $\mathcal{B}_{\text{train}}$ and $\mathcal{B}_{\text{infer}}$ represent the resource budgets for training and inference, respectively. The parameter $\rho$ specifies the maximum tolerable missing rate, ensuring robust performance even under high levels of incomplete data.

\section{Summary}

The central research question addressed in this work is as follows: \textbf{\small How can a modular and unified learning framework be designed to achieve robust prediction performance under severe data incompleteness, while simultaneously reducing training and inference costs for practical deployment?}

This framework tackles the joint objective by seamlessly integrating feature imputation, pruning, architectural tuning, and knowledge distillation into a single pipeline.
Such integration not only enhances robustness and efficiency, but also provides a principled foundation for the subsequent methodological developments presented in this paper.

% ------------------------------------------------
\EndChapter
% ------------------------------------------------
