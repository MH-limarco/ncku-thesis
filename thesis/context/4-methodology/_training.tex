\section{Teacher Training}
\label{sec:training}

After obtaining the pruned GNN from the MPP workflow , a dedicated training phase is conducted to optimize its parameters on the imputed dataset generated by the APCFI module. Specifically, the pruned GNN is trained using the recovered feature matrix $\widehat{\mathbf{X}}$ and the original graph structure, following standard supervised learning protocols for node classification.

Throughout this stage (see Fig.~\ref{fig:teacher_training}), the model benefits from both the lightweight architecture achieved via pruning and the improved data quality resulting from feature imputation. The cross-entropy loss is adopted as the objective function to maximize classification accuracy. Upon completion of training, the resulting high-performance and efficient GNN is designated as the \textbf{Teacher Model} for the subsequent knowledge distillation phase.

This process ensures that the teacher model not only possesses strong predictive capabilities but also maintains computational efficiency, thereby serving as an ideal source of knowledge for transferring to a student model.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\linewidth]{./context/fig/training.png} % 根據實際圖片檔名修改
    \caption{
        \textbf{Teacher Training Workflow.}
        The pruned GNN obtained from the MPP module is fully trained on the imputed feature matrix $\widehat{\mathbf{X}}$ generated by APCFI. The resulting efficient model is used as the Teacher for knowledge distillation.
    }
    \label{fig:teacher_training}
\end{figure}